{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435e1527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "PWD = os.getenv('PWD')\n",
    "os.chdir(PWD)\n",
    "sys.path.insert(0, os.getenv('PWD'))\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"local_settings.py\")\n",
    "import django\n",
    "django.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b0e11d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess,time\n",
    "import numpy as np\n",
    "import cv2,os\n",
    "from AiHandler.wav2lip import audio\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from skimage import img_as_ubyte\n",
    "from django.conf import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf36d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "339ab2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_kp(kp_source, kp_driving, kp_driving_initial, adapt_movement_scale=False,\n",
    "                 use_relative_movement=False, use_relative_jacobian=False):\n",
    "    if adapt_movement_scale:\n",
    "        source_area = ConvexHull(kp_source['value'][0].data.cpu().numpy()).volume\n",
    "        driving_area = ConvexHull(kp_driving_initial['value'][0].data.cpu().numpy()).volume\n",
    "        adapt_movement_scale = np.sqrt(source_area) / np.sqrt(driving_area)\n",
    "    else:\n",
    "        adapt_movement_scale = 1\n",
    "\n",
    "    kp_new = {k: v for k, v in kp_driving.items()}\n",
    "\n",
    "    if use_relative_movement:\n",
    "        kp_value_diff = (kp_driving['value'] - kp_driving_initial['value'])\n",
    "        kp_value_diff *= adapt_movement_scale\n",
    "        kp_new['value'] = kp_value_diff + kp_source['value']\n",
    "\n",
    "        if use_relative_jacobian:\n",
    "            jacobian_diff = torch.matmul(kp_driving['jacobian'], torch.inverse(kp_driving_initial['jacobian']))\n",
    "            kp_new['jacobian'] = torch.matmul(jacobian_diff, kp_source['jacobian'])\n",
    "\n",
    "    return kp_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea15431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wav2lip_datagen(mels,video_path,face_cord_path,start_frame=0,totalFrames=3000,img_size=settings.WAVLIPIMAGESIZE,wav2lip_batch_size=settings.WAVLIPBATCHSIZE):\n",
    "    img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
    "    total_frames = len(mels)\n",
    "    face_cord = np.load(open(face_cord_path,'rb'))[start_frame:start_frame+total_frames]\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    if start_frame>0:\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "    tempFrame=None\n",
    "    img_size_half = img_size//2\n",
    "    currentFrame = start_frame - 1\n",
    "    for i, m in enumerate(mels):\n",
    "        currentFrame +=1\n",
    "        currentIndex = currentFrame%totalFrames\n",
    "        if currentIndex == 0 and currentFrame!=0:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, currentIndex)\n",
    "        is_frame, frame = video.read()\n",
    "        if is_frame:\n",
    "            tempFrame = frame\n",
    "        x1,y1,x2,y2 = face_cord[currentIndex]\n",
    "\n",
    "        img_batch.append(cv2.resize(tempFrame[y1:y2,x1:x2],(img_size,img_size)))\n",
    "        mel_batch.append(m)\n",
    "        frame_batch.append(tempFrame)\n",
    "        coords_batch.append((y1, y2, x1, x2))\n",
    "\n",
    "        if len(img_batch) >= wav2lip_batch_size:\n",
    "            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
    "\n",
    "            img_masked = img_batch.copy()\n",
    "            img_masked[:, img_size_half:] = 0\n",
    "\n",
    "            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
    "            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "            yield img_batch, mel_batch, frame_batch, coords_batch\n",
    "            img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
    "\n",
    "    if len(img_batch) > 0:\n",
    "        img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
    "\n",
    "        img_masked = img_batch.copy()\n",
    "        img_masked[:, img_size_half:] = 0\n",
    "\n",
    "        img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
    "        mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "        yield img_batch, mel_batch, frame_batch, coords_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccd33417",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=settings.DEVICE\n",
    "melstepsize = settings.WAVLIPMELSTEPSIZE\n",
    "mel_idx_multiplier = 80./settings.VIDEO_DEFAULT_FPS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08374ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2lipVideo = \"/home/govind/VideoAutomation/src/private_data/avatars/4/wav2lip/video.mp4\"\n",
    "wav2lipFaceCord = \"/home/govind/VideoAutomation/src/private_data/avatars/4/wav2lip/face_coordinate.npy\"\n",
    "startFrame = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "144adf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audioPath = \"/home/govind/VideoAutomation/src/uploads/usersound/generated/\"\n",
    "allFiles = os.listdir(audioPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d184a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileS = [[os.path.getsize(audioPath + ii),audioPath + ii] for ii in allFiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bf530f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileS = sorted(fileS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edaaec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "_topTenSize = [ii[1] for ii in fileS[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12cb14d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeVideoFileFromFrame(n,pred, frames, coords):\n",
    "    videoWriter = cv2.VideoWriter(f'/home/govind/test/npa/{n}.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 30, (512,512))\n",
    "    for p, f, c in zip(pred, frames, coords):\n",
    "        y1, y2, x1, x2 = c\n",
    "        p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n",
    "        f[y1:y2, x1:x2] = p\n",
    "        videoWriter.write(cv2.resize(cv2.cvtColor(f,cv2.COLOR_BGR2RGB), (512,512))[..., :3])\n",
    "    videoWriter.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0971875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "834c56c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /home/govind/VideoAutomation/src/uploads/usersound/generated/566350c5-6dd2-46db-8a43-8cb0bc2427d9.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/govind/VideoAutomation/.env/lib/python3.8/site-packages/librosa/core/audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
      "  0%|          | 0/54 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mel Chunks Len:  6827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:23<00:00,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for n,_audioP in enumerate(_topTenSize[::-1][:1]):\n",
    "    \n",
    "    print(n,_audioP)\n",
    "    wav = audio.load_wav(_audioP, 16000)\n",
    "    mel = audio.melspectrogram(wav)\n",
    "\n",
    "    if np.isnan(mel.reshape(-1)).sum() > 0:\n",
    "        print('Unable to generate')\n",
    "        break\n",
    "    mel_chunks = []\n",
    "    i = 0\n",
    "    while 1:\n",
    "        start_idx = int(i * mel_idx_multiplier)\n",
    "        if start_idx + melstepsize > len(mel[0]):\n",
    "            mel_chunks.append(mel[:, len(mel[0]) - melstepsize:])\n",
    "            break\n",
    "        mel_chunks.append(mel[:, start_idx : start_idx + melstepsize])\n",
    "        i += 1\n",
    "    mel_chunk_len = len(mel_chunks)\n",
    "    print(\"Mel Chunks Len: \",mel_chunk_len)\n",
    "    \n",
    "    wav2lip_data_generator = wav2lip_datagen(mel_chunks,wav2lipVideo,wav2lipFaceCord,startFrame,3020)\n",
    "    wav2lipOutput = []\n",
    "    currentMaxBatch = 3000\n",
    "    prevTh = None\n",
    "    for i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(wav2lip_data_generator,total=int(np.ceil(float(mel_chunk_len)/settings.WAVLIPBATCHSIZE)))):\n",
    "        with torch.no_grad():\n",
    "            img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(DEVICE)\n",
    "            mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(DEVICE)\n",
    "            pred = settings.WAVLIPMODEL(mel_batch, img_batch)\n",
    "            pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
    "            prevTh = Thread(target=writeVideoFileFromFrame,args=(i,pred,frames,coords,))\n",
    "            prevTh.start()\n",
    "            wav2lipOutput.append(prevTh)\n",
    "            #writeVideoFileFromFrame(videoWriter,pred,frames,coords)\n",
    "        if len(wav2lipOutput)>currentMaxBatch:\n",
    "            wav2lipOutput = []\n",
    "            print('Resetting List')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbe561",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6682e7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5225472000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "63*96*96*3*3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_audioPath = \"\"\n",
    "   \n",
    "wav = audio.load_wav(os.path.join(queue_inst.getcwd(),'sound.wav'), 16000)\n",
    "mel = audio.melspectrogram(wav)\n",
    "\n",
    "if np.isnan(mel.reshape(-1)).sum() > 0:\n",
    "    print('Unable to generate')\n",
    "    break\n",
    "    \n",
    "mel_chunks = []\n",
    "\n",
    "i = 0\n",
    "while 1:\n",
    "    start_idx = int(i * mel_idx_multiplier)\n",
    "    if start_idx + melstepsize > len(mel[0]):\n",
    "        mel_chunks.append(mel[:, len(mel[0]) - melstepsize:])\n",
    "        break\n",
    "    mel_chunks.append(mel[:, start_idx : start_idx + melstepsize])\n",
    "    i += 1\n",
    "mel_chunk_len = len(mel_chunks)\n",
    "\n",
    "\n",
    "## load first and last frame\n",
    "try:\n",
    "    _ = allAvatarImgSeq[avatar_inst.id][0]\n",
    "except:\n",
    "    allAvatarImgSeq[avatar_inst.id] = sorted(os.listdir(os.path.join(avatar_inst.getcwd(),'fullbody/without_swap/')))\n",
    "\n",
    "\n",
    "drivingInit = torch.tensor(np.load(avatar_inst.getFirstInitFrame())).permute(0, 3, 1, 2).to(DEVICE)\n",
    "sourceFrame = torch.tensor(np.load(avatar_inst.getSourceFrame())).permute(0, 3, 1, 2).to(DEVICE)\n",
    "kpSourceFrame = settings.FIRSTORDERKPDETECTOR(sourceFrame)\n",
    "kpDrivingInit = settings.FIRSTORDERKPDETECTOR(drivingInit)\n",
    "\n",
    "startFrameIndx = 0\n",
    "aiStartFrame = queue_inst.start_frame\n",
    "aiOutputFolder = queue_inst.getFaceSwapDir()\n",
    "avatarMainFolder = os.path.join(avatar_inst.getcwd(),'fullbody/without_swap/')\n",
    "\n",
    "# remove existing avatar data\n",
    "try:\n",
    "    os.system(f\"rm -rf {aiOutputFolder}*\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "currentFrame = queue_inst.start_frame - 1\n",
    "\n",
    "wav2lip_data_generator = wav2lip_datagen(mel_chunks,avatar_inst.getWav2lipVideo(),avatar_inst.getFaceCordinate(),queue_inst.start_frame,queue_inst.avatar_image.totalFrames)\n",
    "wav2lipOutput = []\n",
    "currentMaxBatch = 3000\n",
    "for i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(wav2lip_data_generator,total=int(np.ceil(float(mel_chunk_len)/settings.WAVLIPBATCHSIZE)))):\n",
    "    with torch.no_grad():\n",
    "        img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(DEVICE)\n",
    "        mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(DEVICE)\n",
    "        pred = settings.WAVLIPMODEL(mel_batch, img_batch)\n",
    "        pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
    "        for p, f, c in zip(pred, frames, coords):\n",
    "            y1, y2, x1, x2 = c\n",
    "            p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n",
    "            f[y1:y2, x1:x2] = p\n",
    "\n",
    "            drivingData = cv2.resize(cv2.cvtColor(f,cv2.COLOR_BGR2RGB), (512,512))[..., :3].astype('float32')/255\n",
    "            wav2lipOutput.append(drivingData)\n",
    "\n",
    "    if len(wav2lipOutput)>currentMaxBatch:\n",
    "        for drivingData in tqdm(wav2lipOutput,total=len(wav2lipOutput)):\n",
    "            currentFrame += 1\n",
    "            currentIndex = currentFrame%queue_inst.avatar_image.totalFrames\n",
    "            with torch.no_grad():\n",
    "                drivingDataT = torch.tensor(drivingData[np.newaxis]).permute(0,3,1,2).to(DEVICE)\n",
    "                kpDriving = settings.FIRSTORDERKPDETECTOR(drivingDataT)\n",
    "                kpNorm = normalize_kp(kp_source=kpSourceFrame, kp_driving=kpDriving,\n",
    "                                kp_driving_initial=kpDrivingInit, use_relative_movement=True,\n",
    "                                use_relative_jacobian=True, adapt_movement_scale=True)\n",
    "\n",
    "                fout = settings.FIRSTORDERGENERATOR(sourceFrame, kp_source=kpSourceFrame, kp_driving=kpNorm)\n",
    "                aiRGBFrame = img_as_ubyte(np.transpose(fout['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "\n",
    "            aiRGBAFrame = cv2.cvtColor(aiRGBFrame,cv2.COLOR_RGB2RGBA)\n",
    "\n",
    "            if currentIndex == 0 and currentFrame!=0:\n",
    "                aiVideoMask.set(cv2.CAP_PROP_POS_FRAMES, currentIndex)\n",
    "            ret,aiVideoMaskFrame = aiVideoMask.read()\n",
    "            aiRGBAFrame[:,:,3] = aiVideoMaskFrame[:,:,1]\n",
    "            aiRGBAFrame = cv2.resize(aiRGBAFrame,(avatarPosSize,avatarPosSize))\n",
    "            if avatarPosIY<=0:\n",
    "                aiRGBAFrame = aiRGBAFrame[avatarPosIY:avatarPosFY,avatarPosIX:avatarPosFX]\n",
    "            ctAiProcess = Thread(target=saveAiSwapFrame, args=(os.path.join(avatarMainFolder,allAvatarImgSeq[avatar_inst.id][currentIndex]),aiRGBAFrame,(avatarPosIX,avatarPosIY,avatarPosFX,avatarPosFY),os.path.join(aiOutputFolder,f'{str(startFrameIndx).zfill(5)}.png'),))\n",
    "            ctAiProcess.start()\n",
    "            startFrameIndx+=1\n",
    "\n",
    "            ## update progress in db\n",
    "            if (currentProcessingFrame+startFrameIndx)%settings.VIDEO_PROGRESS_UPDATE_FRAME==0:\n",
    "                firstQueueData.updateProgress(currentProcessingFrame+startFrameIndx)\n",
    "\n",
    "        wav2lipOutput = []\n",
    "\n",
    "gc.collect()\n",
    "queue_inst.output = json.dumps({\"wav2lip\": {\"status\": True}})\n",
    "queue_inst.save()\n",
    "print(f'Wav2Lip Completed: {datetime.now()} {queue_inst.id} {firstQueueData.id}')\n",
    "\n",
    "if len(wav2lipOutput)>0:\n",
    "    for drivingData in tqdm(wav2lipOutput,total=len(wav2lipOutput)):\n",
    "        currentFrame += 1\n",
    "        currentIndex = currentFrame%queue_inst.avatar_image.totalFrames\n",
    "        with torch.no_grad():\n",
    "            drivingDataT = torch.tensor(drivingData[np.newaxis]).permute(0,3,1,2).to(DEVICE)\n",
    "            kpDriving = settings.FIRSTORDERKPDETECTOR(drivingDataT)\n",
    "            kpNorm = normalize_kp(kp_source=kpSourceFrame, kp_driving=kpDriving,\n",
    "                            kp_driving_initial=kpDrivingInit, use_relative_movement=True,\n",
    "                            use_relative_jacobian=True, adapt_movement_scale=True)\n",
    "\n",
    "            fout = settings.FIRSTORDERGENERATOR(sourceFrame, kp_source=kpSourceFrame, kp_driving=kpNorm)\n",
    "            aiRGBFrame = img_as_ubyte(np.transpose(fout['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "\n",
    "        aiRGBAFrame = cv2.cvtColor(aiRGBFrame,cv2.COLOR_RGB2RGBA)\n",
    "        if currentIndex == 0 and currentFrame!=0:\n",
    "            aiVideoMask.set(cv2.CAP_PROP_POS_FRAMES, currentIndex)\n",
    "        ret,aiVideoMaskFrame = aiVideoMask.read()\n",
    "        aiRGBAFrame[:,:,3] = aiVideoMaskFrame[:,:,1]\n",
    "        aiRGBAFrame = cv2.resize(aiRGBAFrame,(avatarPosSize,avatarPosSize))\n",
    "        if avatarPosIY<=0:\n",
    "            aiRGBAFrame = aiRGBAFrame[avatarPosIY:avatarPosFY,avatarPosIX:avatarPosFX]\n",
    "        ctAiProcess = Thread(target=saveAiSwapFrame, args=(os.path.join(avatarMainFolder,allAvatarImgSeq[avatar_inst.id][currentIndex]),aiRGBAFrame,(avatarPosIX,avatarPosIY,avatarPosFX,avatarPosFY),os.path.join(aiOutputFolder,f'{str(startFrameIndx).zfill(5)}.png'),))\n",
    "        ctAiProcess.start()\n",
    "        startFrameIndx+=1\n",
    "\n",
    "        ## update progress in db\n",
    "        if (currentProcessingFrame+startFrameIndx)%settings.VIDEO_PROGRESS_UPDATE_FRAME==0:\n",
    "            firstQueueData.updateProgress(currentProcessingFrame+startFrameIndx)\n",
    "\n",
    "\n",
    "        #if (min(currentProcessingFrame/firstQueueData.totalFrames,1)*100)%\n",
    "    gc.collect()\n",
    "aiVideoMask.release()\n",
    "allThread.append(ctAiProcess)\n",
    "queue_inst.status = 1\n",
    "queue_inst.output = json.dumps({\"first_order\": {\"status\": True}})\n",
    "queue_inst.save()\n",
    "\n",
    "currentProcessingFrame += queue_inst.totalOutputFrame\n",
    "firstQueueData.updateProgress(currentProcessingFrame)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
